EKS setup using EKSCTL, ALB Ingress Controller setup, Cluster Autoscaler setup and Sample Helm chart for Application deployment:
===============================================================================================================================
Prequisites:
------------
1) Install AWS CLI
      Linux,Windows - https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html

2) Install eksctl
      Linux,Windows - https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html
	  
	  Linux:
	     curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
	     sudo mv /tmp/eksctl /usr/local/bin
	     eksctl version
	  
3) Install helm
      Linux:
         $ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
         $ chmod 700 get_helm.sh
         $ ./get_helm.sh

4) Install kubectl
      Linux,Windows - https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html

      Linux:
	     Kubernetes 1.20 -  curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.20.4/2021-04-12/bin/linux/amd64/kubectl
		                chmod +x ./kubectl
                                mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin
                                echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
                                kubectl version --short --client
							
5) Install Git
      Linux,Windows - https://www.atlassian.com/git/tutorials/install-git
	  
      Linux: 
	     $ sudo apt-get update
         $ sudo apt-get install git
         $ git --version
        

EKS setup using EKSCTL:
-----------------------

https://eksctl.io/usage/creating-and-managing-clusters/

1) Create a file(ex: cluster-config.yaml) and paste below configurations. Change the VPC and Subnet id's in the file.

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: demo-cluster-eksctl-prod
  region: ap-south-1
  version: "1.19"
vpc:
  id: vpc-0c4e4648ffafe36a8
  subnets:
    public:
      public-1a:
        id: subnet-0c5a7b67fc928a2e8
      public-1b:
        id: subnet-0b20f2858291fb4b6
    private:
      private-1a:
        id: subnet-01e1c53f586267b25
      private-1b:
        id: subnet-095913b978b8b2a65
managedNodeGroups:
  - name: prod-v2
    instanceType: t2.medium
    desiredCapacity: 1
    privateNetworking: true
    volumeSize: 20
    volumeType: gp3
    subnets:
      - private-1a
      - private-1b
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
      withAddonPolicies:
        imageBuilder: true
        autoScaler: true
        certManager: true
        ebs: true
        albIngress: true
        cloudWatch: true
		
2) Run the below eksctl command with the file.
 
      eksctl create cluster -f cluster-config.yaml
	  
	  
ALB Ingress Controller setup:
-----------------------------

https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/deploy/installation/

1) Add tags in subnets. Change the cluster name in the tags.

       Public:
           kubernetes.io/role/elb: 1
           kubernetes.io/cluster/${your-cluster-name}: owned
   
       Private:
           kubernetes.io/role/internal-elb: 1
           kubernetes.io/cluster/${your-cluster-name}: owned

2) Run below command to add eks chart
      helm repo add eks https://aws.github.io/eks-charts

3) Run below command to install the TargetGroupBinding CRDs
      kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
	  
4) Run below command to install ALB ingress controller. Change the cluster name in the command
      helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=<cluster-name>
		  


Sample Helm chart for Application deployment:
---------------------------------------------

1) Create a namespace and update namespace name in values.yaml

            kubectl create ns dev

2) Create a helm chart in the server

            helm create demo-helm-chart
					
3) Change the application name in our sample helm chart using the below sed command(replacing MAN with ThisIsTooLargePleaseJustify using below sed command)

            grep -RirwIl 'MAN' | xargs sed -i 's/\bMAN\b/ThisIsTooLargePleaseJustify/gI'  
			
3) Delete the follwing files in the chart and add our sample files or remove and paste our chart data in mentioned files.(values.yaml, deployment.yaml, service.yaml, hpa.yaml, ingress.yaml)  


5) Install the helm chart with the application name and chart name

           helm install <applicationnameAny> <chartname>
		   
		   
		   

Cluster Autoscaler setup:
-------------------------###############################################

https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html

1) Create a file(cluster-autoscalar.yaml). Change the cluster name at last in the configuration file.

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.17.3
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<YOUR CLUSTER NAME>
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt #/etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
			
2) Run the below command to apply the create file after changing cluster name in file.

          kubectl apply -f cluster-autoscalar.yaml
		  
		  
3) Increase the maximum number of nodes in the nodegroup. By default it will have 1 as maxmium nodes in nodegroup.(if we created cluster using eksctl) 


4) Change the maxmium pods scaleup in hpa.yaml or update hpa in cluster. Later change the maximum to normal value.

          kubectl edit hpa <hpaname> -n <namespace> 
		  

5) Check if the cluster is scaling up and down after application deployed. Scaleup using below command.
     
	      kubectl scale deploy <deploymentname> --replicas 20 -n <namespace>
